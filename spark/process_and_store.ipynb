{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_broker = \"localhost:9092\" #Yet to figure out how the endpoint links to spark\n",
    "bucket_prefix = \"my-company-bucket-prefix-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Locations\n",
    "\n",
    "To write the Delta table, we need 3 settings: the location of the delta table, the location of the checkpoints and the location of the schema file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_bucket = \"/mnt/10ac-batch-5/week9/g3\"\n",
    "bucket = \"/mnt/10ac-batch-5/week9/g3/speech-to-text-delta\"\n",
    "\n",
    "\n",
    "delta_location = bucket + \"/delta-table\"\n",
    "checkpoint_location = bucket + \"/checkpoints\"\n",
    "schema_location = bucket + \"/kafka_schema.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Schema\n",
    "- Assuming the streaming data from kafka is in json format. To properly read this data into spark, we have to provide a schema.\n",
    "- For efficiency, we will infer the schema one and save it to an s3 location so that every time we save data into the delta lake, we only have to infer rather than re-reading the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyspark==3.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'delta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/wacira/10 Academy/week 9/repositories/Speech_to_text_data_pipeline/spark/process_and_store.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wacira/10%20Academy/week%209/repositories/Speech_to_text_data_pipeline/spark/process_and_store.ipynb#ch0000012?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wacira/10%20Academy/week%209/repositories/Speech_to_text_data_pipeline/spark/process_and_store.ipynb#ch0000012?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/wacira/10%20Academy/week%209/repositories/Speech_to_text_data_pipeline/spark/process_and_store.ipynb#ch0000012?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdelta\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtables\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'delta'"
     ]
    }
   ],
   "source": [
    "## Making necessary imports\n",
    "import json, os, re\n",
    "\n",
    "# from delta.tables import *\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "from delta.tables import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Method to infer the schema of kafka topic and return it in the json format\n",
    "\n",
    "def infer_topic_schema_json(topic):\n",
    "\n",
    "    df_json = (spark.read\n",
    "               .format(\"kafka\")\n",
    "               .option(\"kafka.bootstrap.servers\", kafka_broker)\n",
    "               .option(\"subscribe\", topic)\n",
    "               .option(\"startingOffsets\", \"earliest\")\n",
    "               .option(\"endingOffsets\", \"latest\")\n",
    "               .option(\"failOnDataLoss\", \"false\")\n",
    "               .load()\n",
    "               # filter out empty values\n",
    "               .withColumn(\"value\", expr(\"string(value)\"))\n",
    "               .filter(col(\"value\").isNotNull())\n",
    "               # get latest version of each record\n",
    "               .select(\"key\", expr(\"struct(offset, value) r\"))\n",
    "               .groupBy(\"key\").agg(expr(\"max(r) r\")) \n",
    "               .select(\"r.value\"))\n",
    "    \n",
    "    # decode the json values\n",
    "    df_read = spark.read.json(\n",
    "      df_json.rdd.map(lambda x: x.value), multiLine=True)\n",
    "    \n",
    "    # drop corrupt records\n",
    "    if \"_corrupt_record\" in df_read.columns:\n",
    "        df_read = (df_read\n",
    "                   .filter(col(\"_corrupt_record\").isNotNull())\n",
    "                   .drop(\"_corrupt_record\"))\n",
    " \n",
    "    return df_read.schema.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to write the schema to aws. We will be wriring this schema in the *schema_location* as determined in the *setting up loations* section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dbutils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/wacira/10 Academy/week 9/repositories/Speech_to_text_data_pipeline/spark/process_and_store.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wacira/10%20Academy/week%209/repositories/Speech_to_text_data_pipeline/spark/process_and_store.ipynb#ch0000018?line=7'>8</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/wacira/10%20Academy/week%209/repositories/Speech_to_text_data_pipeline/spark/process_and_store.ipynb#ch0000018?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m infer_schema:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/wacira/10%20Academy/week%209/repositories/Speech_to_text_data_pipeline/spark/process_and_store.ipynb#ch0000018?line=10'>11</a>\u001b[0m   \u001b[39m# topic_schema_txt = infer_topic_schema_json(topic)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/wacira/10%20Academy/week%209/repositories/Speech_to_text_data_pipeline/spark/process_and_store.ipynb#ch0000018?line=12'>13</a>\u001b[0m   dbutils\u001b[39m.\u001b[39mfs\u001b[39m.\u001b[39mrm(schema_location) \u001b[39m#Remove the schema that was present\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/wacira/10%20Academy/week%209/repositories/Speech_to_text_data_pipeline/spark/process_and_store.ipynb#ch0000018?line=13'>14</a>\u001b[0m   dbutils\u001b[39m.\u001b[39mfs\u001b[39m.\u001b[39mput(schema_location, topic_schema_txt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dbutils' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "infer_schema = False #No schema available in s3 bucket\n",
    "\n",
    "if not infer_schema:\n",
    "  try:\n",
    "      topic_schema_txt = dbutils.fs.head(schema_location) #Just directly read from the present schemab\n",
    "  except:\n",
    "    infer_schema = True\n",
    "    pass\n",
    "\n",
    "if infer_schema:\n",
    "  # topic_schema_txt = infer_topic_schema_json(topic)\n",
    "  \n",
    "  dbutils.fs.rm(schema_location) #Remove the schema that was present\n",
    "  dbutils.fs.put(schema_location, topic_schema_txt)# Puts a new schema in the specified location\n",
    "\n",
    "\n",
    "  # Topic not yet read from kafka\n",
    "  # Dbutils not yet installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Kafka Stream\n",
    "Here, we will implement the usage of *readStream* which uses strutured streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stream_kafka_topic(topic, schema):\n",
    "    return (spark.readStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", kafka_broker)\n",
    "            .option(\"subscribe\", topic)\n",
    "            .option(\"startingOffsets\", \"earliest\")\n",
    "            .option(\"failOnDataLoss\", \"false\")\n",
    "            .load()\n",
    "            # filter out empty values\n",
    "            .withColumn(\"value\", expr(\"string(value)\"))\n",
    "            .filter(col(\"value\").isNotNull())\n",
    "            .select(\n",
    "              # offset must be the first field, due to aggregation\n",
    "              expr(\"offset as kafka_offset\"),\n",
    "              expr(\"timestamp as kafka_ts\"),\n",
    "              expr(\"string(key) as kafka_key\"),\n",
    "              \"value\"\n",
    "            )\n",
    "            # get latest version of each record\n",
    "            .select(\"kafka_key\", expr(\"struct(*) as r\"))\n",
    "            .groupBy(\"kafka_key\")\n",
    "            .agg(expr(\"max(r) r\"))\n",
    "            # convert to JSON with schema\n",
    "            .withColumn('value', \n",
    "                        from_json(col(\"r.value\"), topic_schema))\n",
    "            .select('r.kafka_key', \n",
    "                    'r.kafka_offset', \n",
    "                    'r.kafka_ts', \n",
    "                    'value.*'\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading into dataframe\n",
    "Now to the part we have all been waiting for!! The actual loading of the data in the format we are used to, *hehehe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_stream_kafka_topic(topic, topic_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Delta Table\n",
    "\n",
    "Now, we have a ready dataframe. We will use the schema of this dataframe to make an empty dataframe. The empty dataframe created will just act as a blueprint to the creation of the delta table ie **The created empty dataframe can be used if a delta table doesn't already exist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark\n",
    " .createDataFrame([], df.schema)\n",
    " .write\n",
    " .option(\"mergeSchema\", \"true\")\n",
    " .format(\"delta\")\n",
    " .mode(\"append\")\n",
    " .save(delta_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up\n",
    "\n",
    "- We start this cleanup stage  early because this code will be executed everytime we run the script. Because we are using streaming we do not know when the update is finished. That’s why we prefer to clean the delta table before we start the streaming process.\n",
    "- This is done by first optimizing the delta table where we coallace smaller files into larger ones *Optimize* then use *bin packing* to produce evenly-balanced data files with respect to their disk size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'delta_location' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/wacira/10 Academy/week 9/repositories/Speech_to_text_data_pipeline/spark/process_and_store.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/wacira/10%20Academy/week%209/repositories/Speech_to_text_data_pipeline/spark/process_and_store.ipynb#ch0000019?line=0'>1</a>\u001b[0m sql \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOPTIMIZE delta.`\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(delta_location)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wacira/10%20Academy/week%209/repositories/Speech_to_text_data_pipeline/spark/process_and_store.ipynb#ch0000019?line=1'>2</a>\u001b[0m spark\u001b[39m.\u001b[39msql(sql)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wacira/10%20Academy/week%209/repositories/Speech_to_text_data_pipeline/spark/process_and_store.ipynb#ch0000019?line=2'>3</a>\u001b[0m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'delta_location' is not defined"
     ]
    }
   ],
   "source": [
    "sql = \"OPTIMIZE delta.`{}`\".format(delta_location)\n",
    "spark.sql(sql)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we vaccuum the delta to table to throw away data that is older than 7 days. This, however would depend with your specification of the days you'd rather have data retained in the delta table. Delta table grow way too large if at all we do not vaccuum it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"VACUUM delta.`{}`\".format(delta_location)\n",
    "spark.sql(sql)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
