{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_broker = \"localhost:9092\" #Yet to figure out how the endpoint links to spark\n",
    "bucket_prefix = \"my-company-bucket-prefix-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Locations\n",
    "\n",
    "To write the Delta table, we need 3 settings: the location of the delta table, the location of the checkpoints and the location of the schema file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_bucket = \"/mnt/10ac-batch-5/week9/g3\"\n",
    "bucket = \"/mnt/10ac-batch-5/week9/g3/speech-to-text-delta\"\n",
    "\n",
    "\n",
    "delta_location = bucket + \"/delta-table\"\n",
    "checkpoint_location = bucket + \"/checkpoints\"\n",
    "schema_location = bucket + \"/kafka_schema.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Schema\n",
    "- Assuming the streaming data from kafka is in json format. To properly read this data into spark, we have to provide a schema.\n",
    "- For efficiency, we will infer the schema one and save it to an s3 location so that every time we save data into the delta lake, we only have to infer rather than re-reading the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyspark==3.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'delta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/wacira/10 Academy/week 9/repositories/Speech_to_text_data_pipeline/spark/process_and_store.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wacira/10%20Academy/week%209/repositories/Speech_to_text_data_pipeline/spark/process_and_store.ipynb#ch0000012?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wacira/10%20Academy/week%209/repositories/Speech_to_text_data_pipeline/spark/process_and_store.ipynb#ch0000012?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/wacira/10%20Academy/week%209/repositories/Speech_to_text_data_pipeline/spark/process_and_store.ipynb#ch0000012?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdelta\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtables\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'delta'"
     ]
    }
   ],
   "source": [
    "## Making necessary imports\n",
    "import json, os, re\n",
    "\n",
    "# from delta.tables import *\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "from delta.tables import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Method to infer the schema of kafka topic and return it in the json format\n",
    "\n",
    "def infer_topic_schema_json(topic):\n",
    "\n",
    "    df_json = (spark.read\n",
    "               .format(\"kafka\")\n",
    "               .option(\"kafka.bootstrap.servers\", kafka_broker)\n",
    "               .option(\"subscribe\", topic)\n",
    "               .option(\"startingOffsets\", \"earliest\")\n",
    "               .option(\"endingOffsets\", \"latest\")\n",
    "               .option(\"failOnDataLoss\", \"false\")\n",
    "               .load()\n",
    "               # filter out empty values\n",
    "               .withColumn(\"value\", expr(\"string(value)\"))\n",
    "               .filter(col(\"value\").isNotNull())\n",
    "               # get latest version of each record\n",
    "               .select(\"key\", expr(\"struct(offset, value) r\"))\n",
    "               .groupBy(\"key\").agg(expr(\"max(r) r\")) \n",
    "               .select(\"r.value\"))\n",
    "    \n",
    "    # decode the json values\n",
    "    df_read = spark.read.json(\n",
    "      df_json.rdd.map(lambda x: x.value), multiLine=True)\n",
    "    \n",
    "    # drop corrupt records\n",
    "    if \"_corrupt_record\" in df_read.columns:\n",
    "        df_read = (df_read\n",
    "                   .filter(col(\"_corrupt_record\").isNotNull())\n",
    "                   .drop(\"_corrupt_record\"))\n",
    " \n",
    "    return df_read.schema.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
