{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from IPython.display import Image\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import textract\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('../scripts')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = '../data/preprocessed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(filters):\n",
    "    amharic = []\n",
    "    sub_dir = [DIRECTORY + '/' + i for i in os.listdir(DIRECTORY)]\n",
    "    \n",
    "    for d in sub_dir:\n",
    "        with open(d) as fp:\n",
    "            line = fp.readline()\n",
    "            while line:\n",
    "                sent = ''.join( c for c in line if  c not in filters)\n",
    "                amharic.append( sent.rstrip().strip())\n",
    "                line = fp.readline()\n",
    "\n",
    "    return amharic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8d in position 23: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\10-Accademy\\w9\\Speech_to_text_data_pipeline\\notebooks\\clean_load.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/10-Accademy/w9/Speech_to_text_data_pipeline/notebooks/clean_load.ipynb#ch0000009?line=0'>1</a>\u001b[0m filters \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m!\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m#$\u001b[39m\u001b[39m%\u001b[39m\u001b[39m&()*+,-/:;<=>?@[\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m]^_`\u001b[39m\u001b[39m{\u001b[39m\u001b[39m|}~\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m።”፤፦’፥፣.“‘·\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m—\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/10-Accademy/w9/Speech_to_text_data_pipeline/notebooks/clean_load.ipynb#ch0000009?line=1'>2</a>\u001b[0m amharic \u001b[39m=\u001b[39m get_text(filters)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/10-Accademy/w9/Speech_to_text_data_pipeline/notebooks/clean_load.ipynb#ch0000009?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnum amharic sentences: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(amharic))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/10-Accademy/w9/Speech_to_text_data_pipeline/notebooks/clean_load.ipynb#ch0000009?line=4'>5</a>\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m: amharic})\n",
      "\u001b[1;32mc:\\Users\\user\\10-Accademy\\w9\\Speech_to_text_data_pipeline\\notebooks\\clean_load.ipynb Cell 3'\u001b[0m in \u001b[0;36mget_text\u001b[1;34m(filters)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/10-Accademy/w9/Speech_to_text_data_pipeline/notebooks/clean_load.ipynb#ch0000002?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m sub_dir:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/10-Accademy/w9/Speech_to_text_data_pipeline/notebooks/clean_load.ipynb#ch0000002?line=5'>6</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(d) \u001b[39mas\u001b[39;00m fp:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/10-Accademy/w9/Speech_to_text_data_pipeline/notebooks/clean_load.ipynb#ch0000002?line=6'>7</a>\u001b[0m         line \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mreadline()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/10-Accademy/w9/Speech_to_text_data_pipeline/notebooks/clean_load.ipynb#ch0000002?line=7'>8</a>\u001b[0m         \u001b[39mwhile\u001b[39;00m line:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/10-Accademy/w9/Speech_to_text_data_pipeline/notebooks/clean_load.ipynb#ch0000002?line=8'>9</a>\u001b[0m             sent \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin( c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m line \u001b[39mif\u001b[39;00m  c \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m filters)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\cosal\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_decode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8d in position 23: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "filters = '!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n።”፤፦’፥፣.“‘·\\'—\\t\\n'\n",
    "amharic = get_text(filters)\n",
    "print(\"num amharic sentences: \", len(amharic))\n",
    "\n",
    "data = pd.DataFrame({'text': amharic})\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\10-Accademy\\w9\\Speech_to_text_data_pipeline\\notebooks\\clean_load.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/10-Accademy/w9/Speech_to_text_data_pipeline/notebooks/clean_load.ipynb#ch0000008?line=0'>1</a>\u001b[0m data[\u001b[39m\"\u001b[39m\u001b[39mchar_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [\u001b[39mlen\u001b[39m(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mtext]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data[\"char_length\"] = [len(i) for i in data.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\10-Accademy\\w9\\Speech_to_text_data_pipeline\\notebooks\\clean_load.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/10-Accademy/w9/Speech_to_text_data_pipeline/notebooks/clean_load.ipynb#ch0000007?line=0'>1</a>\u001b[0m fig \u001b[39m=\u001b[39m px\u001b[39m.\u001b[39mhistogram(data, x\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchar_length\u001b[39m\u001b[39m\"\u001b[39m, marginal\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbox\u001b[39m\u001b[39m\"\u001b[39m, nbins\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/10-Accademy/w9/Speech_to_text_data_pipeline/notebooks/clean_load.ipynb#ch0000007?line=1'>2</a>\u001b[0m                    title\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDistribution transcription length\u001b[39m\u001b[39m'\u001b[39m,)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/10-Accademy/w9/Speech_to_text_data_pipeline/notebooks/clean_load.ipynb#ch0000007?line=2'>3</a>\u001b[0m Image(pio\u001b[39m.\u001b[39;49mto_image(fig, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpng\u001b[39;49m\u001b[39m'\u001b[39;49m, width\u001b[39m=\u001b[39;49m\u001b[39m1200\u001b[39;49m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\plotly\\io\\_kaleido.py:133\u001b[0m, in \u001b[0;36mto_image\u001b[1;34m(fig, format, width, height, scale, validate, engine)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[39m# Raise informative error message if Kaleido is not installed\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[39mif\u001b[39;00m scope \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    134\u001b[0m             \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39mImage export using the \"kaleido\" engine requires the kaleido package,\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39mwhich can be installed using pip:\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[39m    $ pip install -U kaleido\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    139\u001b[0m         )\n\u001b[0;32m    141\u001b[0m     \u001b[39m# Validate figure\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[39m# ---------------\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     fig_dict \u001b[39m=\u001b[39m validate_coerce_fig_to_dict(fig, validate)\n",
      "\u001b[1;31mValueError\u001b[0m: \nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n"
     ]
    }
   ],
   "source": [
    "fig = px.histogram(data, x=\"char_length\", marginal=\"box\", nbins=500,\n",
    "                   title='Distribution transcription length',)\n",
    "Image(pio.to_image(fig, format='png', width=1200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_size = data.shape[0]\n",
    "data = data[data['char_length'] > 10]\n",
    "print(f'percentage of data lost after removing less than 10 character transcriptions: '\n",
    "      f'{(((initial_size-data.shape[0])/initial_size) * 100):.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_size = data.shape[0]\n",
    "data = data[data['char_length'] < 125]\n",
    "print(f'percentage of data lost after removing more than 130 character transcriptions: '\n",
    "      f'{(((initial_size-data.shape[0])/initial_size) * 100):.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(data, x=\"char_length\", marginal=\"box\", nbins=500,\n",
    "                   title='Distribution of cleaned transcription length')\n",
    "Image(pio.to_image(fig, format='png', width=1200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_data = pd.DataFrame(' '.join(data['text']).split())\n",
    "words_in_data.columns = ['word']\n",
    "words_data = words_in_data.groupby(['word']).agg({'word': 'count'})\n",
    "words_data.columns = ['counts']\n",
    "words_data.reset_index(inplace=True)\n",
    "words_data = words_data.sort_values(\"counts\", ascending=False)\n",
    "words_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.helper.save_csv('../data/amharic_corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cosal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41c6ebc6205276a60830f21a1cf85a85972776fb046ae821d7832f20d8b7fcb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
