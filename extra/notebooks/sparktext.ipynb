{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/09 20:59:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "               .appName('SparkByExamples.com') \\\n",
    "               .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"James\",\"\",\"Smith\",30,\"M\",60000),\n",
    "        (\"Michael\",\"Rose\",\"\",50,\"M\",70000),\n",
    "        (\"Robert\",\"\",\"Williams\",42,\"\",400000),\n",
    "        (\"Maria\",\"Anne\",\"Jones\",38,\"F\",500000),\n",
    "        (\"Jen\",\"Mary\",\"Brown\",45,\"F\",0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- middle_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+---+------+------+\n",
      "|first_name|middle_name|last_name|Age|gender|salary|\n",
      "+----------+-----------+---------+---+------+------+\n",
      "|James     |           |Smith    |30 |M     |60000 |\n",
      "|Michael   |Rose       |         |50 |M     |70000 |\n",
      "|Robert    |           |Williams |42 |      |400000|\n",
      "|Maria     |Anne       |Jones    |38 |F     |500000|\n",
      "|Jen       |Mary       |Brown    |45 |F     |0     |\n",
      "+----------+-----------+---------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"first_name\",\"middle_name\",\"last_name\",\"Age\",\"gender\",\"salary\"]\n",
    "pysparkDF = spark.createDataFrame(data = data, schema = columns)\n",
    "pysparkDF.printSchema()\n",
    "pysparkDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/biniyam_belayneh/week9/Speech_to_text_data_pipeline/extra/notebooks/sparktext.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbini/home/biniyam_belayneh/week9/Speech_to_text_data_pipeline/extra/notebooks/sparktext.ipynb#ch0000001vscode-remote?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mcsv(\u001b[39m\"\u001b[39m\u001b[39m/mnt/10ac-batch-5/week9/reiten/unprocessed/original_set.csv\u001b[39m\u001b[39m\"\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, inferSchema\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbini/home/biniyam_belayneh/week9/Speech_to_text_data_pipeline/extra/notebooks/sparktext.ipynb#ch0000001vscode-remote?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/mnt/10ac-batch-5/week9/reiten/unprocessed/original_set.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ብርሃን ፈይሳየኢትዮጵያ ቦክስ ፌዴሬሽን በየዓመቱ የሚያዘጋጀው የክለቦች ቻ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>የተሻለ ብቃት ያሳዩ ቦክሰኞች ለቶኪዮ ኦሊምፒክ ማጣሪያ ተሳታፊ እንደሚሆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>በቦክስ ስፖርት ከሚካሄዱት ዓመታዊ ቻምፒዮናዎች መካከል አንዱ በክለቦች መ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>የኢትዮጵያ ቦክስ ፌዴሬሽን ከአዲስ አበባ ቦክስ ፌዴሬሽን ጋር በመተባበር...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>በአጠቃላይ ክለቦቻቸውን ወክለውም 12 ሴትና 76 ወንድ ቦክሰኞች በጥቅሉ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Id                                               Text\n",
       "0           0   1  ብርሃን ፈይሳየኢትዮጵያ ቦክስ ፌዴሬሽን በየዓመቱ የሚያዘጋጀው የክለቦች ቻ...\n",
       "1           1   2   የተሻለ ብቃት ያሳዩ ቦክሰኞች ለቶኪዮ ኦሊምፒክ ማጣሪያ ተሳታፊ እንደሚሆ...\n",
       "2           2   3  በቦክስ ስፖርት ከሚካሄዱት ዓመታዊ ቻምፒዮናዎች መካከል አንዱ በክለቦች መ...\n",
       "3           3   4   የኢትዮጵያ ቦክስ ፌዴሬሽን ከአዲስ አበባ ቦክስ ፌዴሬሽን ጋር በመተባበር...\n",
       "4           4   5   በአጠቃላይ ክለቦቻቸውን ወክለውም 12 ሴትና 76 ወንድ ቦክሰኞች በጥቅሉ..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "f2 = pd.read_csv(\"/mnt/10ac-batch-5/week9/reiten/interim/clean_set.csv\")\n",
    "f2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"Collinear Points\")\n",
    "sc = SparkContext('local',conf=conf)    \n",
    "from pyspark.rdd import RDD\n",
    "binary_wave_rdd = sc.binaryFiles('../data/wav/*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(df, id_len = 6, char_limit = 150):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: original dataframe\n",
    "            col_name:\n",
    "            id_len:\n",
    "            char_limit:\n",
    "            \n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        id = 1\n",
    "        st_df = pd.DataFrame(columns = ['Id', 'Text'])\n",
    "        texts = []\n",
    "        ids = []\n",
    "            \n",
    "        for i in range(len(df)):\n",
    "            articles = df.loc[i,\"article\"].split(\"።\")\n",
    "            for text in articles:\n",
    "                if(len(text) < char_limit):\n",
    "                    texts.append(text)\n",
    "                    ids.append(\"0\"*(id_len-(len(str(id))))+str(id))\n",
    "                    id = id+1\n",
    "\n",
    "        st_df['Id'] = ids\n",
    "        st_df['Text'] = texts\n",
    "\n",
    "        return st_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/09 21:03:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"Collinear Points\")\n",
    "sc = SparkContext('local',conf=conf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.rdd import RDD\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io.wavfile import write\n",
    "import scipy.io.wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary rdd file from the audio files\n",
    "binary_wave_rdd = sc.binaryFiles('/mnt/10ac-batch-5/week9/reiten/unprocessed/*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfomer binary_wave_rdd to a tuple rdd with location of file and numpy array\n",
    "rdd = binary_wave_rdd.map(lambda x : (x[0].split('/')[-1].split('.')[0], librosa.load(io.BytesIO(x[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanAudio():\n",
    "    \"\"\"Clean audio data by removing dead spaces, ...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def normalize_audio(self, signal):\n",
    "        feats_mean = np.mean(signal, axis=0)\n",
    "        feats_std = np.std(signal, axis=0)\n",
    "        signal = (signal - feats_mean) / (feats_std + 1e-14)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predict():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.clean_audio = CleanAudio()\n",
    "\n",
    "    def get_audio(self, audio_file):\n",
    "        sr = 8000\n",
    "        wav, rate = audio_file\n",
    "        y = librosa.resample(wav, rate, sr)\n",
    "        return y\n",
    "\n",
    "    def get_clean_audio(self, wav):\n",
    "        y = self.clean_audio.normalize_audio(wav)\n",
    "        return y\n",
    "\n",
    "\n",
    "def validate(rdd):\n",
    "    \n",
    "    predict = Predict()\n",
    "    audio_file_rdd = rdd.map(lambda x : (x[0], predict.get_audio(x[1])))\n",
    "    clean_audio_file_rdd = audio_file_rdd.map(lambda x : (x[0], predict.get_clean_audio(x[1])))\n",
    "    return clean_audio_file_rdd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_rdd, clean_audio_file_rdd = validate(rdd)\n",
    "\n",
    "# get collection of audio wave file and turn it to dictionary\n",
    "coll_clean = clean_audio_file_rdd.collect()\n",
    "dct_clean = dict((y, x) for y, x in coll_clean)\n",
    "\n",
    "# overwrite clean audio to file\n",
    "\n",
    "for i,j in dct_clean.items():\n",
    "    scipy.io.wavfile.write('../data/wav/'+i+'.wav', 8000,j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f0c0d5f4754e70d3732f3977490ca6fe7b2ff7c1a54cd8c7ec9ec06d3768d5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
