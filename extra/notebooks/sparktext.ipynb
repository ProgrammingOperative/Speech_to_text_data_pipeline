{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "               .appName('SparkByExamples.com') \\\n",
    "               .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"James\",\"\",\"Smith\",30,\"M\",60000),\n",
    "        (\"Michael\",\"Rose\",\"\",50,\"M\",70000),\n",
    "        (\"Robert\",\"\",\"Williams\",42,\"\",400000),\n",
    "        (\"Maria\",\"Anne\",\"Jones\",38,\"F\",500000),\n",
    "        (\"Jen\",\"Mary\",\"Brown\",45,\"F\",0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"first_name\",\"middle_name\",\"last_name\",\"Age\",\"gender\",\"salary\"]\n",
    "pysparkDF = spark.createDataFrame(data = data, schema = columns)\n",
    "pysparkDF.printSchema()\n",
    "pysparkDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/mnt/10ac-batch-5/week9/reiten/unprocessed/original_set.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "f2 = pd.read_csv(\"/mnt/10ac-batch-5/week9/reiten/interim/clean_set.csv\")\n",
    "f2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"Collinear Points\")\n",
    "sc = SparkContext('local',conf=conf)    \n",
    "from pyspark.rdd import RDD\n",
    "binary_wave_rdd = sc.binaryFiles('../data/wav/*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(df, id_len = 6, char_limit = 150):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: original dataframe\n",
    "            col_name:\n",
    "            id_len:\n",
    "            char_limit:\n",
    "            \n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        id = 1\n",
    "        st_df = pd.DataFrame(columns = ['Id', 'Text'])\n",
    "        texts = []\n",
    "        ids = []\n",
    "            \n",
    "        for i in range(len(df)):\n",
    "            articles = df.loc[i,\"article\"].split(\"።\")\n",
    "            for text in articles:\n",
    "                if(len(text) < char_limit):\n",
    "                    texts.append(text)\n",
    "                    ids.append(\"0\"*(id_len-(len(str(id))))+str(id))\n",
    "                    id = id+1\n",
    "\n",
    "        st_df['Id'] = ids\n",
    "        st_df['Text'] = texts\n",
    "\n",
    "        return st_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"Collinear Points\")\n",
    "sc = SparkContext('local',conf=conf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.rdd import RDD\n",
    "import librosa\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io.wavfile import write\n",
    "import scipy.io.wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary rdd file from the audio files\n",
    "binary_wave_rdd = sc.binaryFiles('/mnt/10ac-batch-5/week9/reiten/unprocessed/*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfomer binary_wave_rdd to a tuple rdd with location of file and numpy array\n",
    "rdd = binary_wave_rdd.map(lambda x : (x[0].split('/')[-1].split('.')[0], librosa.load(io.BytesIO(x[1]))))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanAudio():\n",
    "    \"\"\"Clean audio data by removing dead spaces, ...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def normalize_audio(self, signal):\n",
    "        feats_mean = np.mean(signal, axis=0)\n",
    "        feats_std = np.std(signal, axis=0)\n",
    "        signal = (signal - feats_mean) / (feats_std + 1e-14)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predict():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.clean_audio = CleanAudio()\n",
    "\n",
    "    def get_audio(self, audio_file):\n",
    "        sr = 8000\n",
    "        wav, rate = audio_file\n",
    "        y = librosa.resample(wav, rate, sr)\n",
    "        return y\n",
    "\n",
    "    def get_clean_audio(self, wav):\n",
    "        y = self.clean_audio.normalize_audio(wav)\n",
    "        return y\n",
    "\n",
    "\n",
    "def validate(rdd):\n",
    "    \n",
    "    predict = Predict()\n",
    "    audio_file_rdd = rdd.map(lambda x : (x[0], predict.get_audio(x[1])))\n",
    "    clean_audio_file_rdd = audio_file_rdd.map(lambda x : (x[0], predict.get_clean_audio(x[1])))\n",
    "    return clean_audio_file_rdd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_audio_file_rdd = validate(binary_wave_rdd)\n",
    "\n",
    "# get collection of audio wave file and turn it to dictionary\n",
    "coll_clean = clean_audio_file_rdd.collect()\n",
    "dct_clean = dict((y, x) for y, x in coll_clean)\n",
    "\n",
    "# overwrite clean audio to file\n",
    "\n",
    "for i,j in dct_clean.items():\n",
    "    scipy.io.wavfile.write('/mnt/10ac-batch-5/week9/reiten/unprocessed/'+i+'.wav', 8000,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/09 21:51:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "               .appName('SparkByExamples.com') \\\n",
    "               .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------------------+\n",
      "| _c0|   _c1|                 _c2|\n",
      "+----+------+--------------------+\n",
      "|null|    Id|                Text|\n",
      "|   0|000001|ብርሃን ፈይሳየኢትዮጵያ ቦክ...|\n",
      "|   1|000002| የተሻለ ብቃት ያሳዩ ቦክሰ...|\n",
      "|   2|000003|በቦክስ ስፖርት ከሚካሄዱት ...|\n",
      "|   3|000004| የኢትዮጵያ ቦክስ ፌዴሬሽን...|\n",
      "|   4|000005| በአጠቃላይ ክለቦቻቸውን ወ...|\n",
      "|   5|000006| ውድድሩ የሚካሄደው በራስ ...|\n",
      "|   6|000007| የቦክስ ስፖርት ከንክኪ ስ...|\n",
      "|   7|000008| በመሆኑም በዚህ ረገድ ውድ...|\n",
      "|   8|000009| ይኸውም ተወዳዳሪዎችና አሰ...|\n",
      "|   9|000010| ከዚህ ባሻገር ውድድር የሚ...|\n",
      "|  10|000011| ኮቪድ 19 ዓለም አቀፍ ወ...|\n",
      "|  11|000012| ከእነዚህ ውድድሮች መካከል...|\n",
      "|  12|000013| ውድድሮች ወደ እንቅስቃሴ ...|\n",
      "|  13|000014| ኢትዮጵያም በዚህ ቻምፒዮና...|\n",
      "|  14|000015| ቻምፒዮናው ጥር 4/2013...|\n",
      "|  15|000016|                null|\n",
      "|  16|000017| የአዲስ ዘመን ጋዜጣ ቀደም...|\n",
      "|  17|000018|እኛም ከእነዚህ ዘገባዎች ጋ...|\n",
      "|  18|000019| በዛሬው የአዲስ ዘመን ዱሮ...|\n",
      "+----+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5 = spark.read.csv(\"/mnt/10ac-batch-5/week9/reiten/interim/clean_set.csv\")\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df5.rdd.map(lambda loop: (\n",
    "      loop[\"Id\"],loop[\"Text\"])\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f0c0d5f4754e70d3732f3977490ca6fe7b2ff7c1a54cd8c7ec9ec06d3768d5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
